{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes datasets peft"
      ],
      "metadata": {
        "id": "IgbRGCgkvdTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Yy_C4e7jvgVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTi1buhTvXAJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "TEACHER_ID = \"Qwen/Qwen2.5-72B-Instruct\"\n",
        "STUDENT_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "TEMPERATURE = 2.0\n",
        "ALPHA = 0.5\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 2\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "print(\"Loading Teacher Model (4-bit)...\")\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_ID)\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    TEACHER_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "teacher_model.eval()\n",
        "\n",
        "print(\"Loading Student Model...\")\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(STUDENT_ID)\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\n",
        "    STUDENT_ID,\n",
        "    torch_dtype=torch.bfloat16\n",
        ").to(DEVICE)\n",
        "student_model.train()\n",
        "\n",
        "def distillation_loss_fn(student_logits, teacher_logits, labels):\n",
        "    # Soften logits using Temperature\n",
        "    soft_targets = F.softmax(teacher_logits / TEMPERATURE, dim=-1)\n",
        "    soft_prob = F.log_softmax(student_logits / TEMPERATURE, dim=-1)\n",
        "\n",
        "    # KL Divergence + standard Cross-Entropy\n",
        "    distill_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (TEMPERATURE**2)\n",
        "    student_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
        "\n",
        "    return (ALPHA * distill_loss) + ((1 - ALPHA) * student_loss)\n",
        "\n",
        "class GoldenDataset(Dataset):\n",
        "    def __init__(self, csv_path, tokenizer):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data.iloc[idx]['Text Chunk']\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=MAX_LENGTH,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return encodings['input_ids'].squeeze(), encodings['attention_mask'].squeeze()\n",
        "\n",
        "\n",
        "def run_distillation(csv_file):\n",
        "    dataset = GoldenDataset(csv_file, student_tokenizer)\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    optimizer = AdamW(student_model.parameters(), lr=5e-5)\n",
        "\n",
        "    print(f\"Starting distillation on {len(dataset)} samples...\")\n",
        "    for epoch in range(3):\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(loader):\n",
        "            input_ids, attention_mask = [b.to(DEVICE) for b in batch]\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_output = teacher_model(input_ids, attention_mask=attention_mask)\n",
        "                teacher_logits = teacher_output.logits\n",
        "\n",
        "\n",
        "            student_output = student_model(input_ids, attention_mask=attention_mask)\n",
        "            student_logits = student_output.logits\n",
        "\n",
        "\n",
        "            loss = distillation_loss_fn(student_logits, teacher_logits, input_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Complete. Avg Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "\n",
        "    student_model.save_pretrained(\"./distilled_qwen_1.5b\")\n",
        "    print(\"Distilled model saved to ./distilled_qwen_1.5b\")\n",
        "\n",
        "run_distillation(\"/content/upsc_dataset_langchain_20260213_095213.csv.csv\")"
      ]
    }
  ]
}